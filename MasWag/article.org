#+TITLE: 複数マシンでの実験を Ansible とかでシュっとした話 — 実験編
#+AUTHOR: MasWag

* はじめに

皆さん実験ぶん回してますか？アルゴリズムを設計する人は勿論ですが、そうでない人も諸々のチューニングをして高速化をするとき、複数手法を比較して選定するとき、実験することは少なくないと思います。実験すると言うのは簡単ですが、色々な組み合わせで試そうとするとかなり長い時間がかかることもあるので、あまり気軽に実験を回せない人も少なくないと思います。

しかし今はクラウドコンピューティングの時代です! 各マシンで完全に独立な実験を回す場合、N 台のマシンで実験を回せば 基本的には 1/N の時間で実験が終わることになります。特に Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)等 クラウドの場合、多くの場合料金計算が 時間 $\times$ 台数なので、 台数を増やしてもその分時間が減れば、なんと料金的には変わらないです。

とは言ったものの複数台のマシンで大量に実験を回すのは一苦労です。実験するマシンの調達、実験するマシンの環境構築、実験タスクの各マシンへの分配、実験結果の回収方法、等々考える必要のある作業は多数あります。この辺りの作業で手間取ると無駄にクラウドの料金が増えてしまうので悲しいです。一方で、よくあるクラウドでの複数台管理についての情報は、「同じ働きをする」マシンを複数台立ててロードバランサ等でタスクを分配する、Webアプリケーション向けのものが殆どなので、そのまま並列実験に適用できないことが多いです。

この文章では、複数マシンで実験を回す方法についての説明をします。内容としてはクラウドを意識はしていますが、大半の内容はオンプレミス環境などでも動作するはずです。特に特定のクラウドに特化した話は全くしていません。例えばAWSではオンデマンドインスタンスで所謂「定価」で計算資源を買うよりも、スポットインスタンスでオークション形式で買った方が安いですが、そういった話はしません。また、この文章で説明している手法は単なる一例にすぎません。「僕の考えた最強の手法」ですらなく、今回の実験をそれなりの労力で「シュっとさせた」というだけなので改善の余地は多いにあります。一方で他の人がこういう実験をどういう風に回しているのかを知りたい、というニーズに答えられていれば幸いです。

** おことわり

今回の文章中に挙げているスクリプトですが、比較的やっつけで書いたものも少なくないのでポータビリティについてはあんまり自信がないです。特に Arch Linux や macOS + Homebrew などの、比較的最新版に近いコマンドラインツールがインストールされる環境で確動作確認を行なっているので、ややバージョンの古めのUbuntuなどでは上手く動作しないかもしれません。

関連した本やブログ記事などがあれば良いのですが、筆者の知る限りでは似たことをやっている文書は見つかりませんでした。御存じの方が居たら教えていただけると参考になります。また技術選定については完全に筆者の好みです。他の例えば全体的にpythonに統一する、など別の方法も可能ですし、そういった方法と比べて今回の方法が優れているかどうかは一概には言えないでしょう。

* 基本方針

今回の実験ではおおよそ以下の様なことを基本方針としています。

- できるだけ自動化させるが、諸々のコストが大きい場合は手動作業も厭わない (KISSの原則)
- できるだけ実験を行なった環境・コマンドの情報を残しておく (再現性の担保)

** できるだけ自動化させるが、諸々のコストが大きい場合は手動作業も厭わない

人類皆自動化大好きですし、勿論僕も好きです。特に多数のマシンの管理を行なう際に手動の作業が多いと手間ですし、手動で色々やって時間がかかるとクラウドの従量課金額が増えてしまうのでシンプルに勿体ないです。そもそもこの記事のタイトルにある Ansible はマシンの構成を自動で管理するツールですし、自動化は行ないます。

しかし、だからと言って全ての作業の自動化はこの記事でのゴールではありません。行き過ぎた自動化によって構築するシステムが複雑すぎるものになってしまうのは考えものです。構築するシステムが複雑になればなるほどデバッグは大変になりますし、本当にやりたい実験に到達するまでの時間が長くなってしまいます。今回の最終目的は自分のやりたい実験を簡単に回すことなのでその上で大きな障壁とならない部分はできるだけシンプルに、場合によっては手動作業で行ないます。

** できるだけ実験を行なった環境・コマンドの情報を残しておく

比較的大規模な実験をする上でとても気がかりなことの一つは、正しい手順で実験をやっているかどうかだと思います。複数の設定で実験をやっていると =git clone= 等でバージョンを変更するのを忘れる、コンパイルを忘れるなどのオペレーションミスが起きる (または起きていないか段々自信がなくなってくる) と思います。

もう一つの大きな懸念事項は実験環境を忘れてしまうということです。一つ一つの実験について、例えば gcc のコンパイルオプションが =-O3= だったか =-O2= だったかを人間が覚えていくのはなかなか難しいですし、自分が書いたソフトウェアの細かいバージョンと実験結果の対応関係は、ちゃんと記録を残しておかないとまずわからなくなってしまうでしょう。

これらの理由で過去に行なった実験自体の正しさに確信が持てない場合、つまり「そういえばこの実験ってどういう環境でやったんだっけ…?」という状況になってしまった場合、最終的には再実験を行なって「正しい」実験を再度行なうことになると思います。大規模な実験となると時間もかかりますし、クラウドで行なうととお金もかかってしまいます。特に時間を置いて新しい環境と比較実験を行ないたい場合には、過去の実験環境の詳細がわかっていない場合には再実験が必要になると思われます。

こういった問題を起こさないためにも、できるだけ実験を行なった環境・コマンドの情報を残しておく、つまり同じ実験を再現させられるようにしておく、ということは今回の実験では重視します。

** COMMENT 具体的には実験スクリプト + 実験対象のGit revisionを残していく
*** 本当はマシンスペック (AWSならインスタンスタイプ) とか OSとかもスクリプトで残せると良いが、難しい場合はテキストで残しても良い


* イカれたメンバーを紹介するぜ!!

- マシンの作成 :: 手動!!
- マシンの起動 :: vmctl!!
- 構成管理 :: Ansible!!
- 実験スクリプト、実験結果の管理 :: Git!!
- 対話的な諸々の自動化 :: expect!!
- 実験開始・終了のお知らせ :: slack!!

以上!!

ここから各登場ツールについて説明をしていきます。なお、最終的なワークフローは 図 hoge の様になります。

[[file:./figs/flowchart.pdf]]


** vmctl

*vmctl* (https://github.com/MasWag/vmctl) は様々な仮想マシンに対して同様のインターフェースで起動、停止などの基本的な操作を行なえる様にした、手製のshell scriptです。「様々な仮想マシン」と言いつつ現状Amazon AWSの ec2 と Virtual boxにしか対応していないのですが、原理的にはコマンドラインインターフェースが用意されている仮想マシンであればそれなりの工数で追加できることになっています。

*** vagrant使えば良くないか?

*** 今回は単にインスタンスのidを毎回打ちたくないというだけの理由で使った
*** Machine名を連番にすることで shellの連番展開が使えて便利

*** TODO 代替品

** Ansible

*** https://www.ansible.com/

*** TODO 代替品

**** 構成管理ツールは色々なものがあるので、好きなものを使えば良いと思う

**** itamae

**** Chef

**** Puppet

** Git

*** https://git-scm.com/

*** 有名分散バージョン管理システム

*** マージが簡単にできるという点が並列実験システムにおいて重要

*** 複数マシンで同時に実験した結果を上手い具合にマージできる

*** 2019年現在でGitがほぼデファクトスタンダードなので使っている

*** 代替品

**** 他のバージョン管理システムでも良いが...

**** ローカルで完結しているものはだめ。例えばRCSとかは良くない

**** 変更の情報をちゃんと追えない (例えば過去の版に戻れない)ものはだめ。例えばDropboxとかrsyncとか。

**** マージが簡単にできないのは良くない。例えばsubversionは良くない

** expect

*** https://core.tcl-lang.org/expect/index

*** Tclで 書かれている、対話的なCUIプログラムの自動化ツール

**** 例えばsshとかftpとかだとどうしても対話的な操作が必要なので、シェルスクリプトで自動化ができない

**** Expectはターミナルの入力を自動化できるので、こういったものも自動化てきる

*** 代替品

**** 今時はTclじゃなくてpythonとかrubyとかで類似のものが出ている

**** sshに特化したFabricとかでも良い

** slack

*** https://slack.com/intl/ja-jp/
* マシン構成パート

** マシンの作成・初期設定: 手動

まず始めに実験に使うマシンを作成して、ユーザ設定や最低限必要な設定などを行ないます。今回はメインの実験で MATLABが必要で、ライセンス管理の自動化が厄介なので、 MATLABのインストールまでを手動でやりました。必要なソフトウェアにライセンス等の問題が全くないのであればこの工程は自動化しても良いと思います。例えば、 Packer で予め必要なマシンイメージを作っておいて、必要な時に必要なだけ Terraform とかでマシンを作って、不要になったらすぐ壊す、ということが可能です。

** TODO vmctl の設定ファイルをjsonで書く

* 実験環境設定パート

** 構成管理 by ansible
*** 言わずと知れた超有名構成管理ツール
*** YAMLでマシンの設定を書くことができる
*** 複数マシンに対して実行することができる
*** 今回は例えば実験スクリプトの入っているgitレポジトリの clone/pullとかに使っている
** 実験内容、実験結果の管理 Git
*** 実験内容の管理としては、単にスクリプトなのでGitで管理すると何かと便利
*** 実験結果の管理としては、複数マシンで生成されたファイルが衝突したときに、ちゃんと対応できる
** 実験結果の解析 bash
*** これは趣味

* 実験パート

** 対話的な諸々の自動化 expect
*** sshが絡むと色々対話的に実行したくなることがある
*** ansible でもできるかもしれないが、知らないうちにオーバーヘッドが載ったりすると嫌なのでできるだけ簡素な方法を使った
*** expectはTclのもの以外にも色々な言語に移植されているので、好きな言語のものを使えば良い
** 解析対象のファイル名指定 YAML
** 解析 Makefile
* 実験後
** 実験開始・終了のお知らせ slack
*** 実験がいつ終わるのか、待つのは精神衛生上よろしくない
** コマンドラインのデータ処理ツール datamash
*** TSV, CSVファイルから平均とか分散とか統計情報を計算できる
*** Pivotingして 人が読めるテーブルを作る
* 全体のワークフロー (フローチャートを書く)

- (ここからマシン構築パート) マシンを作成・設定する (起動させっぱなしだとお金が勿体ないので停止させておく)
   - 今回ここは完全に手動なので特に言うことなし
- vmctl の設定ファイルを構成する
- (ここから実験環境設定パート。人が書く) 実行したい実験の環境を ansible playbookで書く
   - 例えばデータセット、実装などをダウンロードする
   - このplaybookを各実験毎に書くか、一回書いて使い回すかは、各実験で何が変わるかに依る
     - 例えば環境は同じで色々なパラメタに対して実験を回すのであれば、一回書いて使い回せる
     - 旧実装と新実装を比較するなら、少なくともansibleのgitモジュールなどで実装をダウンロードする箇所は使い回せない
       - 上手く作って変数にしておけば大丈夫かも?
   - 諸々のデータはGitHub等十分強いリモートからダウンロードするのが良い
     - そうしないと帯域の関係でローカルからデータを送る部分がボトルネックになりうる
- 実験スクリプトを書く
   - メインパートは、例えば「調査対象のコマンド・関数を実行させて、その前後の時間を計測」など
   - 今回はshell scriptを使ったが他の方法でも良い
   - 実装の版が特に規定されていない (例えばGitのmasterを使う)場合は gitの版のハッシュ値をテキストで保存しておく
   - 実験の開始・終了をslackにお知らせする
   - 実験結果をgit commitする
   - 実験の終了後に自動でインスタンスを停止する
- (ここから実験パート。スクリプトを回す) インスタンスを立てる
   - vmctl を使う
   - Bashの brace expansionを使うと便利 (POSIX標準ではないので、例えばdashでは使えないらしい)
     - 例えば machine1, machine2, machine3, machine4, machine5 を立てるなら =vmctl start machine{1..5}= で良い
     - 詳しくは Bashマニュアル(i.e., =man bash=)のBrace Expansionを参照
- インスタンス上に実験環境を構築する
   - 要するにansibleを叩く
   - ansibleのhostファイルを生成する必要がある
   - リモートでsshを使う場合はssh-agentのforward agentが必要な場合がある
     - https://qiita.com/isaoshimizu/items/84ac5a0b1d42b9d355cf
   - known_hosts問題
     - =export ANSIBLE_HOST_KEY_CHECKING=False=
- 実験スクリプトを回す
   - expect + sshで各インスタンスで実験スクリプトを回す
   - screenを使うことで、ローカルからの接続が切れても問題ないようにする
     - nohupを使っても良いが、screenだと万が一標準エラー出力とかを見たくなったときに簡単に対応できる
- (ここから実験解析パート) インスタンスを立てる
- Gitで実験結果をsyncさせる
- 解析スクリプトを回す

* 今後の課題
** スクリプトのポータビリティを上げる (POSIX shell で動く)
*** 今のスクリプトはdashとかでは動かない
* 次回予告
** どの実験結果を最終的に比較するか、はベタ書きしない
*** 例えば表にして比較する実験設定をスクリプトにベタ書きしない。できるだけDRYにする。
